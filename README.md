# Open Source, 3D Printable, Modular Bipedal Robot Project

The **Modular Bipedal Robot** project aims to educate and inspire individuals interested in robotics and electronics. This open-source initiative focuses on creating a fully autonomous companion robot with a variety of advanced features.

## Key Features

- **Bipedal Design**: The robot includes articulated legs for bipedal movement.
- **Control Systems**: Utilizes Arduino and Raspberry Pi, managed through custom PCBs.
- **Modular Body**: Configurable body components allow for easy customization and adaptability.
- **Software Modules**:
  - Animation: Handles the animation of the robot, including walking, turning, and other movements.
  - Braillespeak: Converts text to Braille and speaks it using a proprietary audio output using the onboard buzzer.
  - Buzzer: Controls the buzzer for audio output. Includes the ability to play tones and melodies.
  - ChatGPT: Uses the OpenAI GPT models to process text based on user input.
  - Logging: Logs data to a file for debugging and analysis.
  - Motion Detection: Handles motion detection using an onboard microwave motion sensor.
  - Neopixel: Controls the onboard Neopixel LEDs for visual feedback.
  - PiServo: Controls the servos connected to the Raspberry Pi.
  - PiTemperature: Reads the temperature from the integrated temperature sensor on the Raspberry Pi.
  - RTLSDR: Uses an RTL-SDR dongle to receive and process radio signals.
  - Serial Connection: Handles serial communication between the Raspberry Pi and Arduino.
  - Servos: Controls the servos connected to the Arduino via the Raspberry Pi and the serial connection.
  - Tracking: Uses computer vision to track objects and faces using the onboard camera.
  - Translator: Translates text between languages using the Google Translate API.
  - TTS: Converts text to speech using the onboard speaker.
  - Viam: Uses the VIAM API to integrate Viam modules for additional functionality.
  - Vision: Handles image processing and computer vision tasks using the onboard IMX500 Raspberry Pi AI camera.
  - [Read more](https://github.com/makerforgetech/modular-biped/wiki/Software#modules)!

## Project Background

The Modular Biped Robot Project is designed to provide a flexible and modular framework for robotics development using Python and C++ on the Raspberry Pi and Arduino platforms. It aims to enable developers, robotics enthusiasts, and curious individuals to experiment, create, and customize their own biped robots. With a range of features and functionalities and the option to add your own easily, the Modular Biped Robot Project offers an exciting opportunity to explore the world of robotics.

## Modularity

The open source framework is designed for flexibility, allowing users to easily add or remove components to suit their specific needs. Comprehensive [guides](https://github.com/makerforgetech/modular-biped/wiki/Software#creating-a-module) are provided for integrating new modules seamlessly.

## Resources

- **Documentation**: For detailed information, visit the projectâ€™s GitHub wiki: [Modular Biped Documentation](https://github.com/makerforgetech/modular-biped/wiki)
- **Code**: Check out the modular open source software on [GitHub](https://github.com/makerforgetech/modular-biped)
- **YouTube Playlist**: Explore the development process through our build videos: [Watch on YouTube](https://www.youtube.com/watch?v=2DVJ5xxAuWY&list=PL_ua9QbuRTv6Kh8hiEXXVqywS8pklZraT)
- **Community**: Have a question or want to show off your build? Join the communities on [GitHub](https://bit.ly/maker-forge-community) and [Discord](https://bit.ly/makerforge-community)!

# Remote Vision System

This system allows offloading computer vision processing from the robot to a remote laptop/desktop computer.

## System Architecture

- **Robot**: Runs a camera streaming service and command receiver
- **Laptop/Desktop**: Runs DeskGUI application that processes video streams and controls the robot

## Setup Instructions

### Robot Side Setup

1. Install dependencies:
   ```bash
   pip install opencv-python numpy pypubsub pyyaml
   ```

2. Update configuration in `config/robot_camera.yml` if needed

3. Run the camera streaming service:
   ```bash
   python run_robot_camera.py
   ```

### Laptop/Desktop Side Setup

1. Install dependencies:
   ```bash
   pip install opencv-python numpy pypubsub pyyaml PyQt5 face_recognition requests
   ```

2. Update configuration in `config/deskgui.yml` if needed

3. Run the DeskGUI application:
   ```bash
   python run_deskgui.py [robot_ip]
   ```
   - Replace `[robot_ip]` with the IP address of your robot (optional, can also be configured in the GUI)

## Usage

1. Launch the DeskGUI application
2. Enter the robot's IP address and click "Connect"
3. Use the control panel to:
   - Toggle vision processing modes (Face Detection, Motion Detection)
   - Enable/disable tracking
   - Control robot movement
   - Run animations
   - Send speech commands
   - Play sounds
   - Train face recognition models

## Vision Processing Modes

- **Face Detection**: Detects and recognizes faces in the video stream
- **Motion Detection**: Detects movement in the video stream
- **Tracking**: When enabled, automatically moves the robot to follow detected faces or motion

## Ollama LLM Integration

The system integrates with Ollama for natural language processing:

1. Enter text in the input field
2. Click "Send to LLM"
3. The response will appear in the output field

## Face Recognition Model Training

To train the face recognition model:

1. Create a `dataset` folder with subfolders for each person
2. Add face images to each person's folder
3. Click "Train Face Recognition Model" in the DeskGUI

## Folder Structure
